{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ_wkNVdTz-C"
   },
   "source": [
    "# Let's train HyperNeRF!\n",
    "\n",
    "**Author**: [Keunhong Park](https://keunhong.com)\n",
    "\n",
    "[[Project Page](https://hypernerf.github.io)]\n",
    "[[Paper](https://arxiv.org/abs/2106.13228)]\n",
    "[[GitHub](https://github.com/google/hypernerf)]\n",
    "\n",
    "This notebook provides an demo for training HyperNeRF.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Convert a video into our dataset format using the Nerfies [dataset processing notebook](https://colab.sandbox.google.com/github/google/nerfies/blob/main/notebooks/Nerfies_Capture_Processing.ipynb).\n",
    "2. Set the `data_dir` below to where you saved the dataset.\n",
    "3. Come back to this notebook to train HyperNeRF.\n",
    "\n",
    "\n",
    "### Notes\n",
    " * To accomodate the limited compute power of Colab runtimes, this notebook defaults to a \"toy\" version of our method. The number of samples have been reduced and the elastic regularization turned off.\n",
    "\n",
    " * To train a high-quality model, please look at the CLI options we provide in the [Github repository](https://github.com/google/hypernerf).\n",
    "\n",
    "\n",
    "\n",
    " * Please report issues on the [GitHub issue tracker](https://github.com/google/hypernerf/issues).\n",
    "\n",
    "\n",
    "If you find this work useful, please consider citing:\n",
    "```bibtex\n",
    "@article{park2021hypernerf\n",
    "  author    = {Park, Keunhong and Sinha, Utkarsh and Hedman, Peter and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Martin-Brualla, Ricardo and Seitz, Steven M.},\n",
    "  title     = {HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields},\n",
    "  journal   = {arXiv preprint arXiv:2106.13228},\n",
    "  year      = {2021},\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlW1gF_djH6H"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTING.md  README.md  eval.py    notebooks\t setup.py\r\n",
      "LICENSE\t\t configs    hypernerf  requirements.txt  train.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/saurabh.nair/giant_nerfs/hypernerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6Jbspl7TnIX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/saurabh.nair/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: flax in /usr/local/lib/python3.8/dist-packages (0.4.1)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.8/dist-packages (2.2.0)\n",
      "Requirement already satisfied: mediapy in /usr/local/lib/python3.8/dist-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.8/dist-packages (from flax) (1.22.3)\n",
      "Requirement already satisfied: optax in /usr/local/lib/python3.8/dist-packages (from flax) (0.1.1)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from flax) (1.0.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from flax) (3.5.1)\n",
      "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.8/dist-packages (from flax) (0.3.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from mediapy) (9.1.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from mediapy) (8.2.0)\n",
      "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from optax->flax) (0.1.2)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax->flax) (0.3.5)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax->flax) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from optax->flax) (4.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (4.31.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (0.11.0)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3->flax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3->flax) (1.4.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (2.11.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (3.0.29)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (0.1.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython->mediapy) (45.2.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (0.2.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (0.2.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.0.4->optax->flax) (0.11.2)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.0.4->optax->flax) (0.1.7)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.8/dist-packages (from jaxlib>=0.1.37->optax->flax) (2.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from absl-py>=0.7.1->optax->flax) (1.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->mediapy) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython->mediapy) (0.7.0)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->mediapy) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->mediapy) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->mediapy) (0.2.2)\n",
      "\u001b[33mWARNING: The directory '/home/saurabh.nair/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Processing /home/saurabh.nair/giant_nerfs/hypernerf\n",
      "Building wheels for collected packages: hypernerf\n",
      "  Building wheel for hypernerf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hypernerf: filename=hypernerf-0.0.1-py3-none-any.whl size=73696 sha256=1ce7341e7781b78281e33866a2adbe64ad85f23c008d9821e7a866e32be721ee\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bf0wgn8i/wheels/56/be/f9/4691bf22fd7609631ab8e12ca6d65643f5d6ff2821b2e52f89\n",
      "Successfully built hypernerf\n",
      "Installing collected packages: hypernerf\n",
      "  Attempting uninstall: hypernerf\n",
      "    Found existing installation: hypernerf 0.0.1\n",
      "    Uninstalling hypernerf-0.0.1:\n",
      "      Successfully uninstalled hypernerf-0.0.1\n",
      "Successfully installed hypernerf-0.0.1\n",
      "\u001b[33mWARNING: The directory '/home/saurabh.nair/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting google-colab\n",
      "  Downloading google-colab-1.0.0.tar.gz (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 5.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting google-auth~=1.4.0\n",
      "  Downloading google_auth-1.4.2-py2.py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 18.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipykernel~=4.6.0\n",
      "  Downloading ipykernel-4.6.1-py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 43.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipython~=5.5.0\n",
      "  Downloading ipython-5.5.0-py3-none-any.whl (758 kB)\n",
      "\u001b[K     |████████████████████████████████| 758 kB 48.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting notebook~=5.2.0\n",
      "  Downloading notebook-5.2.2-py2.py3-none-any.whl (8.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.0 MB 22.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas~=0.24.0\n",
      "  Downloading pandas-0.24.2.tar.gz (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 18.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting portpicker~=1.2.0\n",
      "  Downloading portpicker-1.2.0.tar.gz (17 kB)\n",
      "Collecting requests~=2.21.0\n",
      "  Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 38.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six~=1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tornado~=4.5.0\n",
      "  Downloading tornado-4.5.3.tar.gz (484 kB)\n",
      "\u001b[K     |████████████████████████████████| 484 kB 42.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth~=1.4.0->google-colab) (4.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth~=1.4.0->google-colab) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth~=1.4.0->google-colab) (0.2.8)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel~=4.6.0->google-colab) (5.1.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel~=4.6.0->google-colab) (7.2.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (0.7.5)\n",
      "Collecting simplegeneric>0.8\n",
      "  Downloading simplegeneric-0.8.1.zip (12 kB)\n",
      "Collecting prompt-toolkit<2.0.0,>=1.0.4\n",
      "  Downloading prompt_toolkit-1.0.18-py3-none-any.whl (245 kB)\n",
      "\u001b[K     |████████████████████████████████| 245 kB 41.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (2.11.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython~=5.5.0->google-colab) (45.2.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (6.4.5)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (4.9.2)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (5.3.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (0.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (3.1.1)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from pandas~=0.24.0->google-colab) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.8/dist-packages (from pandas~=0.24.0->google-colab) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.8/dist-packages (from pandas~=0.24.0->google-colab) (2022.1)\n",
      "Collecting urllib3<1.25,>=1.21.1\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 51.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests~=2.21.0->google-colab) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests~=2.21.0->google-colab) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests~=2.21.0->google-colab) (2019.11.28)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa>=3.1.4->google-auth~=1.4.0->google-colab) (0.4.8)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel~=4.6.0->google-colab) (0.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel~=4.6.0->google-colab) (1.5.5)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel~=4.6.0->google-colab) (22.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect; sys_platform != \"win32\"->ipython~=5.5.0->google-colab) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython~=5.5.0->google-colab) (0.2.5)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (2.1.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.5.13)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (4.10.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (5.0.0)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.6.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.8.4)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook~=5.2.0->google-colab) (2.15.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook~=5.2.0->google-colab) (4.4.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->nbconvert->notebook~=5.2.0->google-colab) (2.3.2)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->notebook~=5.2.0->google-colab) (0.5.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook~=5.2.0->google-colab) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook~=5.2.0->google-colab) (21.4.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook~=5.2.0->google-colab) (5.6.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema>=2.6->nbformat->notebook~=5.2.0->google-colab) (3.8.0)\n",
      "Building wheels for collected packages: google-colab, pandas, portpicker, tornado, simplegeneric\n",
      "  Building wheel for google-colab (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-colab: filename=google_colab-1.0.0-py2.py3-none-any.whl size=102275 sha256=f6ceb4c993d93d3774f3590a902c0672c0475356a5aba8b404b87348c150fdb0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-koviejhu/wheels/e4/e7/91/b2736701bca00e273fdc79f80e2727b558e0903d81b758eb69\n",
      "  Building wheel for pandas (setup.py) ... \u001b[?25l-"
     ]
    }
   ],
   "source": [
    "!pip3 install flax immutabledict mediapy\n",
    "#!pip3 install --upgrade git+https://github.com/google/hypernerf\n",
    "### Set the local hypernerf path with the Jax version fix!\n",
    "!pip3 install /home/saurabh.nair/giant_nerfs/hypernerf\n",
    "!pip3 install google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "zGJux-m5Xp3Z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Devices: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# @title Configure notebook runtime\n",
    "# @markdown If you would like to use a GPU runtime instead, change the runtime type by going to `Runtime > Change runtime type`. \n",
    "# @markdown You will have to use a smaller batch size on GPU.\n",
    "\n",
    "runtime_type = 'gpu'  # @param ['gpu', 'tpu']\n",
    "if runtime_type == 'tpu':\n",
    "  import jax.tools.colab_tpu\n",
    "  jax.tools.colab_tpu.setup_tpu()\n",
    "import jax\n",
    "print('Detected Devices:', jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "afUtLfRWULEi"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# @title Mount Google Drive\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# @markdown Mount Google Drive onto `/content/gdrive`. You can skip this if running locally.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      5\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# @title Mount Google Drive\n",
    "# @markdown Mount Google Drive onto `/content/gdrive`. You can skip this if running locally.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "ENOfbG3AkcVN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/jax/_src/config.py:171: UserWarning: enable_omnistaging() is a no-op in JAX versions 0.2.12 and higher;\n",
      "see https://github.com/google/jax/blob/main/design_notes/omnistaging.md\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# @title Define imports and utility functions.\n",
    "\n",
    "import jax\n",
    "from jax.config import config as jax_config\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax import jax_utils\n",
    "from flax import optim\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import checkpoints\n",
    "jax_config.enable_omnistaging() # Linen requires enabling omnistaging\n",
    "\n",
    "from absl import logging\n",
    "from io import BytesIO\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import PIL\n",
    "import IPython\n",
    "\n",
    "\n",
    "# Monkey patch logging.\n",
    "def myprint(msg, *args, **kwargs):\n",
    " print(msg % args)\n",
    "\n",
    "logging.info = myprint \n",
    "logging.warn = myprint\n",
    "logging.error = myprint\n",
    "\n",
    "\n",
    "def show_image(image, fmt='png'):\n",
    "    image = image_utils.image_to_uint8(image)\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(image).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW7FsSB-jORB"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rz7wRm7YT9Ka"
   },
   "outputs": [],
   "source": [
    "# @title Model and dataset configuration\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import gin\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from hypernerf import models\n",
    "from hypernerf import modules\n",
    "from hypernerf import warping\n",
    "from hypernerf import datasets\n",
    "from hypernerf import configs\n",
    "\n",
    "\n",
    "# @markdown The working directory.\n",
    "train_dir = '/content/gdrive/My Drive/nerfies/hypernerf_experiments/capture1/exp1'  # @param {type: \"string\"}\n",
    "# @markdown The directory to the dataset capture.\n",
    "data_dir = '/content/gdrive/My Drive/nerfies/captures/capture1'  # @param {type: \"string\"}\n",
    "\n",
    "# @markdown Training configuration.\n",
    "max_steps = 100000  # @param {type: 'number'}\n",
    "batch_size = 4096  # @param {type: 'number'}\n",
    "image_scale = 8  # @param {type: 'number'}\n",
    "\n",
    "# @markdown Model configuration.\n",
    "use_viewdirs = True  #@param {type: 'boolean'}\n",
    "use_appearance_metadata = True  #@param {type: 'boolean'}\n",
    "num_coarse_samples = 64  # @param {type: 'number'}\n",
    "num_fine_samples = 64  # @param {type: 'number'}\n",
    "\n",
    "# @markdown Deformation configuration.\n",
    "use_warp = True  #@param {type: 'boolean'}\n",
    "warp_field_type = '@SE3Field'  #@param['@SE3Field', '@TranslationField']\n",
    "warp_min_deg = 0  #@param{type:'number'}\n",
    "warp_max_deg = 6  #@param{type:'number'}\n",
    "\n",
    "# @markdown Hyper-space configuration.\n",
    "hyper_num_dims = 8  #@param{type:'number'}\n",
    "hyper_point_min_deg = 0  #@param{type:'number'}\n",
    "hyper_point_max_deg = 1  #@param{type:'number'}\n",
    "hyper_slice_method = 'bendy_sheet'  #@param['none', 'axis_aligned_plane', 'bendy_sheet']\n",
    "\n",
    "\n",
    "checkpoint_dir = Path(train_dir, 'checkpoints')\n",
    "checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "config_str = f\"\"\"\n",
    "DELAYED_HYPER_ALPHA_SCHED = {{\n",
    "  'type': 'piecewise',\n",
    "  'schedules': [\n",
    "    (1000, ('constant', 0.0)),\n",
    "    (0, ('linear', 0.0, %hyper_point_max_deg, 10000))\n",
    "  ],\n",
    "}}\n",
    "\n",
    "ExperimentConfig.image_scale = {image_scale}\n",
    "ExperimentConfig.datasource_cls = @NerfiesDataSource\n",
    "NerfiesDataSource.data_dir = '{data_dir}'\n",
    "NerfiesDataSource.image_scale = {image_scale}\n",
    "\n",
    "NerfModel.use_viewdirs = {int(use_viewdirs)}\n",
    "NerfModel.use_rgb_condition = {int(use_appearance_metadata)}\n",
    "NerfModel.num_coarse_samples = {num_coarse_samples}\n",
    "NerfModel.num_fine_samples = {num_fine_samples}\n",
    "\n",
    "NerfModel.use_viewdirs = True\n",
    "NerfModel.use_stratified_sampling = True\n",
    "NerfModel.use_posenc_identity = False\n",
    "NerfModel.nerf_trunk_width = 128\n",
    "NerfModel.nerf_trunk_depth = 8\n",
    "\n",
    "TrainConfig.max_steps = {max_steps}\n",
    "TrainConfig.batch_size = {batch_size}\n",
    "TrainConfig.print_every = 100\n",
    "TrainConfig.use_elastic_loss = False\n",
    "TrainConfig.use_background_loss = False\n",
    "\n",
    "# Warp configs.\n",
    "warp_min_deg = {warp_min_deg}\n",
    "warp_max_deg = {warp_max_deg}\n",
    "NerfModel.use_warp = {use_warp}\n",
    "SE3Field.min_deg = %warp_min_deg\n",
    "SE3Field.max_deg = %warp_max_deg\n",
    "SE3Field.use_posenc_identity = False\n",
    "NerfModel.warp_field_cls = @SE3Field\n",
    "\n",
    "TrainConfig.warp_alpha_schedule = {{\n",
    "    'type': 'linear',\n",
    "    'initial_value': {warp_min_deg},\n",
    "    'final_value': {warp_max_deg},\n",
    "    'num_steps': {int(max_steps*0.8)},\n",
    "}}\n",
    "\n",
    "# Hyper configs.\n",
    "hyper_num_dims = {hyper_num_dims}\n",
    "hyper_point_min_deg = {hyper_point_min_deg}\n",
    "hyper_point_max_deg = {hyper_point_max_deg}\n",
    "\n",
    "NerfModel.hyper_embed_cls = @hyper/GLOEmbed\n",
    "hyper/GLOEmbed.num_dims = %hyper_num_dims\n",
    "NerfModel.hyper_point_min_deg = %hyper_point_min_deg\n",
    "NerfModel.hyper_point_max_deg = %hyper_point_max_deg\n",
    "\n",
    "TrainConfig.hyper_alpha_schedule = %DELAYED_HYPER_ALPHA_SCHED\n",
    "\n",
    "hyper_sheet_min_deg = 0\n",
    "hyper_sheet_max_deg = 6\n",
    "HyperSheetMLP.min_deg = %hyper_sheet_min_deg\n",
    "HyperSheetMLP.max_deg = %hyper_sheet_max_deg\n",
    "HyperSheetMLP.output_channels = %hyper_num_dims\n",
    "\n",
    "NerfModel.hyper_slice_method = '{hyper_slice_method}'\n",
    "NerfModel.hyper_sheet_mlp_cls = @HyperSheetMLP\n",
    "NerfModel.hyper_use_warp_embed = True\n",
    "\n",
    "TrainConfig.hyper_sheet_alpha_schedule = ('constant', %hyper_sheet_max_deg)\n",
    "\"\"\"\n",
    "\n",
    "gin.parse_config(config_str)\n",
    "\n",
    "config_path = Path(train_dir, 'config.gin')\n",
    "with open(config_path, 'w') as f:\n",
    "  logging.info('Saving config to %s', config_path)\n",
    "  f.write(config_str)\n",
    "\n",
    "exp_config = configs.ExperimentConfig()\n",
    "train_config = configs.TrainConfig()\n",
    "eval_config = configs.EvalConfig()\n",
    "\n",
    "display(Markdown(\n",
    "    gin.config.markdown(gin.config_str())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "r872r6hiVUVS"
   },
   "outputs": [],
   "source": [
    "# @title Create datasource and show an example.\n",
    "\n",
    "from hypernerf import datasets\n",
    "from hypernerf import image_utils\n",
    "\n",
    "dummy_model = models.NerfModel({}, 0, 0)\n",
    "datasource = exp_config.datasource_cls(\n",
    "    image_scale=exp_config.image_scale,\n",
    "    random_seed=exp_config.random_seed,\n",
    "    # Enable metadata based on model needs.\n",
    "    use_warp_id=dummy_model.use_warp,\n",
    "    use_appearance_id=(\n",
    "        dummy_model.nerf_embed_key == 'appearance'\n",
    "        or dummy_model.hyper_embed_key == 'appearance'),\n",
    "    use_camera_id=dummy_model.nerf_embed_key == 'camera',\n",
    "    use_time=dummy_model.warp_embed_key == 'time')\n",
    "\n",
    "show_image(datasource.load_rgb(datasource.train_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XC3PIY74XB05"
   },
   "outputs": [],
   "source": [
    "# @title Create training iterators\n",
    "\n",
    "devices = jax.local_devices()\n",
    "\n",
    "train_iter = datasource.create_iterator(\n",
    "    datasource.train_ids,\n",
    "    flatten=True,\n",
    "    shuffle=True,\n",
    "    batch_size=train_config.batch_size,\n",
    "    prefetch_size=3,\n",
    "    shuffle_buffer_size=train_config.shuffle_buffer_size,\n",
    "    devices=devices,\n",
    ")\n",
    "\n",
    "def shuffled(l):\n",
    "  import random as r\n",
    "  import copy\n",
    "  l = copy.copy(l)\n",
    "  r.shuffle(l)\n",
    "  return l\n",
    "\n",
    "train_eval_iter = datasource.create_iterator(\n",
    "    shuffled(datasource.train_ids), batch_size=0, devices=devices)\n",
    "val_eval_iter = datasource.create_iterator(\n",
    "    shuffled(datasource.val_ids), batch_size=0, devices=devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erY9l66KjYYW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nZnS8BhcXe5E"
   },
   "outputs": [],
   "source": [
    "# @title Initialize model\n",
    "# @markdown Defines the model and initializes its parameters.\n",
    "\n",
    "from flax.training import checkpoints\n",
    "from hypernerf import models\n",
    "from hypernerf import model_utils\n",
    "from hypernerf import schedules\n",
    "from hypernerf import training\n",
    "\n",
    "# @markdown Restore a checkpoint if one exists.\n",
    "restore_checkpoint = False  # @param{type:'boolean'}\n",
    "\n",
    "\n",
    "rng = random.PRNGKey(exp_config.random_seed)\n",
    "np.random.seed(exp_config.random_seed + jax.process_index())\n",
    "devices_to_use = jax.devices()\n",
    "\n",
    "learning_rate_sched = schedules.from_config(train_config.lr_schedule)\n",
    "nerf_alpha_sched = schedules.from_config(train_config.nerf_alpha_schedule)\n",
    "warp_alpha_sched = schedules.from_config(train_config.warp_alpha_schedule)\n",
    "elastic_loss_weight_sched = schedules.from_config(\n",
    "train_config.elastic_loss_weight_schedule)\n",
    "hyper_alpha_sched = schedules.from_config(train_config.hyper_alpha_schedule)\n",
    "hyper_sheet_alpha_sched = schedules.from_config(\n",
    "    train_config.hyper_sheet_alpha_schedule)\n",
    "\n",
    "rng, key = random.split(rng)\n",
    "params = {}\n",
    "model, params['model'] = models.construct_nerf(\n",
    "      key,\n",
    "      batch_size=train_config.batch_size,\n",
    "      embeddings_dict=datasource.embeddings_dict,\n",
    "      near=datasource.near,\n",
    "      far=datasource.far)\n",
    "\n",
    "optimizer_def = optim.Adam(learning_rate_sched(0))\n",
    "optimizer = optimizer_def.create(params)\n",
    "\n",
    "state = model_utils.TrainState(\n",
    "    optimizer=optimizer,\n",
    "    nerf_alpha=nerf_alpha_sched(0),\n",
    "    warp_alpha=warp_alpha_sched(0),\n",
    "    hyper_alpha=hyper_alpha_sched(0),\n",
    "    hyper_sheet_alpha=hyper_sheet_alpha_sched(0))\n",
    "scalar_params = training.ScalarParams(\n",
    "    learning_rate=learning_rate_sched(0),\n",
    "    elastic_loss_weight=elastic_loss_weight_sched(0),\n",
    "    warp_reg_loss_weight=train_config.warp_reg_loss_weight,\n",
    "    warp_reg_loss_alpha=train_config.warp_reg_loss_alpha,\n",
    "    warp_reg_loss_scale=train_config.warp_reg_loss_scale,\n",
    "    background_loss_weight=train_config.background_loss_weight,\n",
    "    hyper_reg_loss_weight=train_config.hyper_reg_loss_weight)\n",
    "\n",
    "if restore_checkpoint:\n",
    "  logging.info('Restoring checkpoint from %s', checkpoint_dir)\n",
    "  state = checkpoints.restore_checkpoint(checkpoint_dir, state)\n",
    "step = state.optimizer.state.step + 1\n",
    "state = jax_utils.replicate(state, devices=devices)\n",
    "del params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "at2CL5DRZ7By"
   },
   "outputs": [],
   "source": [
    "# @title Define pmapped functions\n",
    "# @markdown This parallelizes the training and evaluation step functions using `jax.pmap`.\n",
    "\n",
    "import functools\n",
    "from hypernerf import evaluation\n",
    "\n",
    "\n",
    "def _model_fn(key_0, key_1, params, rays_dict, extra_params):\n",
    "  out = model.apply({'params': params},\n",
    "                    rays_dict,\n",
    "                    extra_params=extra_params,\n",
    "                    rngs={\n",
    "                        'coarse': key_0,\n",
    "                        'fine': key_1\n",
    "                    },\n",
    "                    mutable=False)\n",
    "  return jax.lax.all_gather(out, axis_name='batch')\n",
    "\n",
    "pmodel_fn = jax.pmap(\n",
    "    # Note rng_keys are useless in eval mode since there's no randomness.\n",
    "    _model_fn,\n",
    "    in_axes=(0, 0, 0, 0, 0),  # Only distribute the data input.\n",
    "    devices=devices_to_use,\n",
    "    axis_name='batch',\n",
    ")\n",
    "\n",
    "render_fn = functools.partial(evaluation.render_image,\n",
    "                              model_fn=pmodel_fn,\n",
    "                              device_count=len(devices),\n",
    "                              chunk=eval_config.chunk)\n",
    "train_step = functools.partial(\n",
    "    training.train_step,\n",
    "    model,\n",
    "    elastic_reduce_method=train_config.elastic_reduce_method,\n",
    "    elastic_loss_type=train_config.elastic_loss_type,\n",
    "    use_elastic_loss=train_config.use_elastic_loss,\n",
    "    use_background_loss=train_config.use_background_loss,\n",
    "    use_warp_reg_loss=train_config.use_warp_reg_loss,\n",
    "    use_hyper_reg_loss=train_config.use_hyper_reg_loss,\n",
    ")\n",
    "ptrain_step = jax.pmap(\n",
    "    train_step,\n",
    "    axis_name='batch',\n",
    "    devices=devices,\n",
    "    # rng_key, state, batch, scalar_params.\n",
    "    in_axes=(0, 0, 0, None),\n",
    "    # Treat use_elastic_loss as compile-time static.\n",
    "    donate_argnums=(2,),  # Donate the 'batch' argument.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vbc7cMr5aR_1"
   },
   "outputs": [],
   "source": [
    "# @title Train!\n",
    "# @markdown This runs the training loop!\n",
    "\n",
    "import mediapy\n",
    "from hypernerf import utils\n",
    "from hypernerf import visualization as viz\n",
    "\n",
    "\n",
    "print_every_n_iterations = 100  # @param{type:'number'}\n",
    "visualize_results_every_n_iterations = 500  # @param{type:'number'}\n",
    "save_checkpoint_every_n_iterations = 1000  # @param{type:'number'}\n",
    "\n",
    "\n",
    "logging.info('Starting training')\n",
    "rng = rng + jax.process_index()  # Make random seed separate across hosts.\n",
    "keys = random.split(rng, len(devices))\n",
    "time_tracker = utils.TimeTracker()\n",
    "time_tracker.tic('data', 'total')\n",
    "\n",
    "for step, batch in zip(range(step, train_config.max_steps + 1), train_iter):\n",
    "  time_tracker.toc('data')\n",
    "  scalar_params = scalar_params.replace(\n",
    "      learning_rate=learning_rate_sched(step),\n",
    "      elastic_loss_weight=elastic_loss_weight_sched(step))\n",
    "  # pytype: enable=attribute-error\n",
    "  nerf_alpha = jax_utils.replicate(nerf_alpha_sched(step), devices)\n",
    "  warp_alpha = jax_utils.replicate(warp_alpha_sched(step), devices)\n",
    "  hyper_alpha = jax_utils.replicate(hyper_alpha_sched(step), devices)\n",
    "  hyper_sheet_alpha = jax_utils.replicate(\n",
    "      hyper_sheet_alpha_sched(step), devices)\n",
    "  state = state.replace(nerf_alpha=nerf_alpha,\n",
    "                        warp_alpha=warp_alpha,\n",
    "                        hyper_alpha=hyper_alpha,\n",
    "                        hyper_sheet_alpha=hyper_sheet_alpha)\n",
    "\n",
    "  with time_tracker.record_time('train_step'):\n",
    "    state, stats, keys, _ = ptrain_step(keys, state, batch, scalar_params)\n",
    "    time_tracker.toc('total')\n",
    "\n",
    "  if step % print_every_n_iterations == 0:\n",
    "    logging.info(\n",
    "        'step=%d, warp_alpha=%.04f, hyper_alpha=%.04f, hyper_sheet_alpha=%.04f, %s',\n",
    "        step, \n",
    "        warp_alpha_sched(step), \n",
    "        hyper_alpha_sched(step), \n",
    "        hyper_sheet_alpha_sched(step), \n",
    "        time_tracker.summary_str('last'))\n",
    "    coarse_metrics_str = ', '.join(\n",
    "        [f'{k}={v.mean():.04f}' for k, v in stats['coarse'].items()])\n",
    "    fine_metrics_str = ', '.join(\n",
    "        [f'{k}={v.mean():.04f}' for k, v in stats['fine'].items()])\n",
    "    logging.info('\\tcoarse metrics: %s', coarse_metrics_str)\n",
    "    if 'fine' in stats:\n",
    "      logging.info('\\tfine metrics: %s', fine_metrics_str)\n",
    "  \n",
    "  if step % visualize_results_every_n_iterations == 0:\n",
    "    print(f'[step={step}] Training set visualization')\n",
    "    eval_batch = next(train_eval_iter)\n",
    "    render = render_fn(state, eval_batch, rng=rng)\n",
    "    rgb = render['rgb']\n",
    "    acc = render['acc']\n",
    "    depth_exp = render['depth']\n",
    "    depth_med = render['med_depth']\n",
    "    rgb_target = eval_batch['rgb']\n",
    "    depth_med_viz = viz.colorize(depth_med, cmin=datasource.near, cmax=datasource.far)\n",
    "    mediapy.show_images([rgb_target, rgb, depth_med_viz],\n",
    "                        titles=['GT RGB', 'Pred RGB', 'Pred Depth'])\n",
    "\n",
    "    print(f'[step={step}] Validation set visualization')\n",
    "    eval_batch = next(val_eval_iter)\n",
    "    render = render_fn(state, eval_batch, rng=rng)\n",
    "    rgb = render['rgb']\n",
    "    acc = render['acc']\n",
    "    depth_exp = render['depth']\n",
    "    depth_med = render['med_depth']\n",
    "    rgb_target = eval_batch['rgb']\n",
    "    depth_med_viz = viz.colorize(depth_med, cmin=datasource.near, cmax=datasource.far)\n",
    "    mediapy.show_images([rgb_target, rgb, depth_med_viz],\n",
    "                       titles=['GT RGB', 'Pred RGB', 'Pred Depth'])\n",
    "\n",
    "  if step % save_checkpoint_every_n_iterations == 0:\n",
    "    training.save_checkpoint(checkpoint_dir, state)\n",
    "\n",
    "  time_tracker.tic('data', 'total')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o69auGWvdyyd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HyperNeRF Training.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
